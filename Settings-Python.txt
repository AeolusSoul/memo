import win32com.client
import subprocess
import time
import os
import pandas as pd
import requests
from datetime import datetime

# --- 設定エリア ---
SOURCE_FILE = r"C:\path\to\labeled_file.xlsx"  # ラベル付きブック
SHEET_NAME = "〇〇"
TEMP_CSV = "temp_data.csv"
OUTPUT_FILE = "result_judgment.xlsx"

def clean_excel_process():
    """実行前にExcelプロセスを完全に掃除"""
    subprocess.run('taskkill /F /IM excel.exe /T', stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, shell=True)
    time.sleep(1)

def export_sheet_to_csv(excel_path, sheet_name, csv_path):
    """ラベル付きブックを開き、特定のシートをCSVで保存する"""
    excel = None
    try:
        excel = win32com.client.DispatchEx("Excel.Application")
        excel.Visible = False
        excel.DisplayAlerts = False
        
        wb = excel.Workbooks.Open(os.path.abspath(excel_path))
        ws = wb.Sheets(sheet_name)
        
        # FileFormat=6 は CSV (UTF-8なら62ですが、Excel互換の6が安定)
        ws.SaveAs(os.path.abspath(csv_path), FileFormat=6)
        
        wb.Close(False)
        print(f"CSV変換成功: {csv_path}")
    finally:
        if excel:
            excel.Quit()
            excel = None

def process_data():
    """pandasでデータを加工し、URL判定を行う"""
    # 1. CSV読み込み (Excel出力のCSVはShift-JISが多い)
    df_raw = pd.read_csv(TEMP_CSV, encoding='shift_jis', header=None)

    # 2. キーワード検索（「業務番号」「業務名」「URL」の座標を探す）
    # df.stack()で全セルを走査し、キーワードの行・列を特定
    def find_coords(keyword):
        res = df_raw[df_raw == keyword].stack().index
        return res[0] if len(res) > 0 else (None, None)

    row_num, col_num = find_coords("業務番号")
    row_name, col_name = find_coords("業務名")
    row_url, col_url = find_coords("URL")

    if row_num is None:
        print("キーワードが見つかりませんでした。")
        return

    # 3. データ範囲の確定
    # キーワードの1つ下の行からデータを取得
    nums = df_raw.iloc[row_num + 1:, col_num].reset_index(drop=True)
    names = df_raw.iloc[row_name + 1:, col_name].reset_index(drop=True)
    urls = df_raw.iloc[row_url + 1:, col_url].reset_index(drop=True)

    # 新しいデータフレームを作成
    new_df = pd.DataFrame({
        '業務番号': nums,
        '業務名': names,
        'URL': urls
    }).dropna(how='all') # 全て空の行は削除

    # 4. 列の挿入（本日日付）
    new_df['判定日'] = datetime.now().strftime("%Y/%m/%d")

    # 5. URLアクセス判定
    def check_access(url):
        if not isinstance(url, str) or not url.startswith("http"):
            return "×"
        try:
            # タイムアウトを短めに設定して高速化
            response = requests.get(url, timeout=5, allow_redirects=True)
            return "〇" if response.status_code == 200 else "×"
        except:
            return "×"

    print("URL判定中...")
    new_df['判定結果'] = new_df['URL'].apply(check_access)

    # 6. 列の並び替え (A:番号, B:名, C:判定結果, D:判定日, E:URL)
    # pandasなら列名のリストを渡すだけで一瞬で入れ替わります
    final_df = new_df[['業務番号', '業務名', '判定結果', '判定日', 'URL']]

    # 7. Excelとして保存
    final_df.to_excel(OUTPUT_FILE, index=False)
    print(f"結果を保存しました: {OUTPUT_FILE}")

# --- 実行 ---
if __name__ == "__main__":
    clean_excel_process() # 掃除
    export_sheet_to_csv(SOURCE_FILE, SHEET_NAME, TEMP_CSV) # 変換
    process_data() # 加工・判定
    
    # 後片付け（一時CSVの削除）
    if os.path.exists(TEMP_CSV):
        os.remove(TEMP_CSV)



import win32com.client
import subprocess
import time
import gc
from datetime import datetime

def run_excel_process():
    # 1. 開始時のプロセス掃除
    subprocess.run('taskkill /F /IM excel.exe /T', stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, shell=True)
    time.sleep(1)

    excel = None
    wb_source = None
    wb_new = None

    try:
        # Excel起動設定
        excel = win32com.client.DispatchEx("Excel.Application")
        excel.Visible = False
        excel.DisplayAlerts = False
        excel.ScreenUpdating = False

        # 2. ラベル付きブックを開き、新規ブックへコピー
        source_path = r"C:\path\to\your_labeled_file.xlsx" # 適切なパスに変更
        wb_source = excel.Workbooks.Open(source_path)
        ws_src = wb_source.Sheets("〇〇") # シート名を指定
        
        data = ws_src.UsedRange.Value # 全データをタプルで取得
        
        wb_new = excel.Workbooks.Add()
        ws_work = wb_new.Sheets(1)
        ws_work.Name = "作業シート"
        
        # 新規ブックに一括貼り付け（ラベルの影響を排除）
        if data:
            ws_work.Range(ws_work.Cells(1, 1), ws_work.Cells(len(data), len(data[0]))).Value = data
        
        # ラベル付きブックは即座に閉じる
        wb_source.Close(False)
        wb_source = None

        # 3. 検索と抽出（新規シート「判定結果」へ集約）
        ws_dest = wb_new.Sheets.Add()
        ws_dest.Name = "判定結果"

        def get_data_by_keyword(keyword):
            # 検索してその1つ下のセルを起点に範囲取得
            found = ws_work.UsedRange.Find(keyword)
            if found:
                # ひとつ下のセルの行・列
                start_row = found.Row + 1
                col = found.Column
                last_row = ws_work.Cells(ws_work.Rows.Count, col).End(-4162).Row # -4162 = xlUp
                if last_row < start_row: return None
                return ws_work.Range(ws_work.Cells(start_row, col), ws_work.Cells(last_row, col)).Value
            return None

        # 業務番号、業務名、URLをそれぞれ取得して A1, B1, C1 へ
        nums = get_data_by_keyword("業務番号")
        names = get_data_by_keyword("業務名")
        urls = get_data_by_keyword("URL")

        if nums: ws_dest.Range(ws_dest.Cells(1, 1), ws_dest.Cells(len(nums), 1)).Value = nums
        if names: ws_dest.Range(ws_dest.Cells(1, 2), ws_dest.Cells(len(names), 2)).Value = names
        if urls: ws_dest.Range(ws_dest.Cells(1, 3), ws_dest.Cells(len(urls), 3)).Value = urls

        # 4. 列挿入と判定処理
        # 現在の状態：A=番号, B=名, C=URL
        last_row_dest = ws_dest.Cells(ws_dest.Rows.Count, 3).End(-4162).Row
        
        # C列に新規列を挿入（日付用）
        # これにより、元のURLは D列に移動する
        ws_dest.Columns("C").Insert() 
        ws_dest.Range(ws_dest.Cells(1, 3), ws_dest.Cells(last_row_dest, 3)).Value = datetime.now().strftime("%Y/%m/%d")
        
        # さらにC列に新規列を挿入（判定結果用：〇×）
        # これにより、日付はD列、URLはE列に移動する
        ws_dest.Columns("C").Insert() 
        
        # 現在の構成：A=番号, B=名, C=空(判定), D=日付, E=URL
        url_col = 5 # E列
        res_col = 3 # C列
        
        # 判定ループ（ここは1行ずつ処理が必要）
        print("URLアクセス判定を開始します...")
        for i in range(1, last_row_dest + 1):
            url = ws_dest.Cells(i, url_col).Value
            if url and str(url).startswith("http"):
                # ここで判定ロジック（例：簡易的なチェック）
                # 実際には requests ライブラリ等でアクセス確認を行う
                is_accessible = True # ここに判定処理を入れる
                ws_dest.Cells(i, res_col).Value = "〇" if is_accessible else "×"

        # 5. 保存と終了
        wb_new.SaveAs(r"C:\path\to\result_file.xlsx")
        print("処理が正常に完了しました。")

    except Exception as e:
        print(f"エラー発生: {e}")
    finally:
        if wb_new: wb_new.Close(False)
        if excel: excel.Quit()
        # 参照解放
        ws_src = None; ws_work = None; ws_dest = None; wb_source = None; wb_new = None; excel = None
        gc.collect()

if __name__ == "__main__":
    run_excel_process()




win32
処理開始時に、プロセス残留や参照エラーを解消し、確実に実行開始する
ラベル付きブックを開く
〇〇シート全体を新規ブックを作成し、タプルでコピペする
ラベル付きブックを確実に終了する
新規ブックの〇〇シートの処理
・業務番号で検索し、該当セルのひとつ下のセルの行番号と列番号を変数に格納し、データ範囲を確定する
データ範囲をコピーし、新規シートa1へペーストする
次に、業務名で検索し、列番号を変数に格納し、データ範囲を、確定する
データ範囲をコピーし、さっきのシートのb1へペーストする
次にURLで検索し、列番を変数に格納、データ範囲をコピーし、さっきのシート
c1へペーストする

ここから、c列に挿入したURLへアクセスし、判定を開始する

さっきのシートのデータ範囲を取得し、変数に格納する
c列に新規列を挿入し、データ範囲に本日日付yyyy/mm/ddを入力する
ここでURL列がc列にからd列に変わってしまうため、どうすればよいか
URLにかかれたリンク先へアクセスし、アクセス可能か判定し、c列に新規列を挿入し、〇、× を入力する

ブックを保存し、安全に終了する




Python Code

LibraryのInstall

cmd:
pip install requests beautifulsoup4

--
Webスクレイピング

制約
・Robots.txt

必要なもの
・データの取得先URL
・欲しい情報のタグ

ライブラリ
  - requests 静的ページのHTML取得
  - httpx 非同期通信
  - BeautifulSoup4 HTMLを構文解析、特定のタグを抽出
  - lxml HTML/XML解析
  - html5lib
  - Selenium JSで生成されるページから取得
  - Playwright ブラウザ操作
  - requests_html JSレンダリング 
  - pandas データを表形式にしてCSV、Excelに保存
  - json JSON形式のAPIレスポンスを扱う
  - csv 取得データをcsvに出力

--
Script

########################################
# Webスクレイピング - 静的サイト単体ページ
########################################

import requests
from bs4 import BeautifulSoup

# デフォルトのUser-Agent
DEFAULT_USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/118.0.0.0 Safari/537.36"
)

def get_html_title(url, user_agent=None):
    """
    指定したURLの<title>タグを取得する関数。

    Parameters:
        url (str): 取得対象のURL
        user_agent (str, optional): User-Agentヘッダー。未指定の場合はデフォルト値を使用。

    Returns:
        str | None: タイトルテキスト（取得できなかった場合はNone）
    """
    headers = {"User-Agent": user_agent or DEFAULT_USER_AGENT}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
    except requests.RequestException as e:
        status_code = getattr(e.response, "status_code", "不明")
        print(f"[Error] URL: {url} | Status: {status_code} | Detail: {e}")
        return None

    soup = BeautifulSoup(res.text, "html.parser")
    title_tag = soup.find("title")
    return title_tag.text.strip() if title_tag else None

# 使用例
if __name__ == "__main__":
    url = "https://tenki.jp/forecast/3/12/"
    title = get_html_title(url)
    if title:
        print("ページタイトル:", title)
    else:
        print("情報を取得できませんでした。")

--

ページネーション（複数ページの処理）
ログインが必要なサイトへのアクセス（Cookie/Session）
スクレイピング＋自動化（SeleniumやPlaywright）
JSON API直接取得（HTML解析不要）
クラウド実行や定期実行（例：cron, Lambda）

--
以上


from openpyxl import Workbook, load_workbook
wb = Workbook()
ws = wb.active
ws["A1"] = "test"
wb.save("test.xlsx")
wb2 = load_workbook("test.xlsx")

from openpyxl import Workbook, load_workbook
path = r"C:\temp\check.xlsx"
wb = Workbook()
wb.save(path)
wb2 = load_workbook(path)
print("OK")

サーバのフォルダパス定義
サーバのフォルダを検索
サーバのファイル名定義
サーバのファイルを検索
存在するファイルの最新更新日を変数に登録
ファイルの有無エラーフラグつける
ローカルの元フォルダ定義
ローカルの先フォルダ定義
ファイル有無エラーなければ、サーバの最新更新日のファイルをローカルの元フォルダへコピー
コピーしたローカルの元ファイルの〇〇シートを読み込み、シート全体コピー、先ファイルに新規シートを追加、シート名workへ変更、A1にペーストする
1シート目の名前変更
今回_更新日_ファイルの最新更新日
2シート目の名前変更
前回_更新日_先フォルダの前回のファイルフォルダのファイルの更新日
項目名の定義
業務番号
業務名
URL
workシート目検索開始
wordシートの業務番号を検索し、行番号、列番号を取得し、変数に格納
業務番号のひとつ下のセルを開始位置とし、最終行までのセル範囲のデータをコピーし、今回シートの業務名のひとつしたのセルを開始位置としてペースト
同様に、同じ行を業務名で検索し、セル範囲のデータをＢ3へペースト
同様に、同じ行を左から初期照査で検索し、その位置から最もURLの項目を検索し、セル範囲のデータを〇〇へペースト
元ファイル閉じる
先ファイル上書き保存して閉じる

前の関数から引き継ぐもの
開始行番号
列番号
セル範囲
各URLの列番号

先ファイルを読み込む

セル範囲のアクセス日に本日の日付yyyy/mm/ddを入力

セル範囲の初期照査URL項目の入力有無判定
入力なしの場合、アクセス日からURLまでの間のセルにはいふんを入力

入力ある場合、フォルダかファイルのURLか末尾の拡張子の有無で判別

ファイルの場合、
ファイル有無に◯を入力
ファイル・フォルダの合計に1を入力
更新日にファイルの更新日を入力

フォルダの場合、
ファイル有無にフォルダ内ファイル検索し、あれば◯、なければ✖️
ファイル・フォルダの合計に、フォルダ内のファイルとフォルダをカウント、合計数を入力
更新日にフォルダ内にあるファイルで最もファイルの更新日が最新のファイルの更新日を入力


これらを変数に持たせておいて、
最後にまとめてエクセルに書き出す




Python Code

LibraryのInstall

cmd:
pip install requests beautifulsoup4

--
Webスクレイピング

制約
・Robots.txt

必要なもの
・データの取得先URL
・欲しい情報のタグ

ライブラリ
  - requests 静的ページのHTML取得
  - httpx 非同期通信
  - BeautifulSoup4 HTMLを構文解析、特定のタグを抽出
  - lxml HTML/XML解析
  - html5lib
  - Selenium JSで生成されるページから取得
  - Playwright ブラウザ操作
  - requests_html JSレンダリング 
  - pandas データを表形式にしてCSV、Excelに保存
  - json JSON形式のAPIレスポンスを扱う
  - csv 取得データをcsvに出力

--
Script

########################################
# Webスクレイピング - 静的サイト単体ページ
########################################

import requests
from bs4 import BeautifulSoup

# デフォルトのUser-Agent
DEFAULT_USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/118.0.0.0 Safari/537.36"
)

def get_html_title(url, user_agent=None):
    """
    指定したURLの<title>タグを取得する関数。

    Parameters:
        url (str): 取得対象のURL
        user_agent (str, optional): User-Agentヘッダー。未指定の場合はデフォルト値を使用。

    Returns:
        str | None: タイトルテキスト（取得できなかった場合はNone）
    """
    headers = {"User-Agent": user_agent or DEFAULT_USER_AGENT}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
    except requests.RequestException as e:
        status_code = getattr(e.response, "status_code", "不明")
        print(f"[Error] URL: {url} | Status: {status_code} | Detail: {e}")
        return None

    soup = BeautifulSoup(res.text, "html.parser")
    title_tag = soup.find("title")
    return title_tag.text.strip() if title_tag else None

# 使用例
if __name__ == "__main__":
    url = "https://tenki.jp/forecast/3/12/"
    title = get_html_title(url)
    if title:
        print("ページタイトル:", title)
    else:
        print("情報を取得できませんでした。")

--

ページネーション（複数ページの処理）
ログインが必要なサイトへのアクセス（Cookie/Session）
スクレイピング＋自動化（SeleniumやPlaywright）
JSON API直接取得（HTML解析不要）
クラウド実行や定期実行（例：cron, Lambda）

--
以上




Python Settings

Install

python.org > download  > Install
Install先：C:\Users\xxx\AppData\Local\Programs\Python\Python314

--
PATH通す

タスクバー > 検索バー > システムの詳細設定 > 詳細設定 > 環境変数　> xxのユーザー環境変数 PATH 選択 > システム環境変数 PATH 選択 > システム環境変数 編集選択
> 新規
> C:\Users\xxx\AppData\Local\Programs\Python\Python314
> 新規 
C:\Users\xxx\AppData\Local\Programs\Python\Python314\Scripts
> OK

PowerShell
cmd:
python --vesion
python -V
# Python 3.14.0

--
以上